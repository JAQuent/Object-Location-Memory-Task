---
title: "Analysis of subject 2"
author: "Joern Alexander Quent"
date: "28/03/2021"
output: html_document
---

```{r setup, include = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE,  warning = FALSE, message = FALSE)
```

```{r lib_and_param}
# Libs
library(ggplot2)
library(plyr)
library(assortedRFunctions)
library(knitr)
library(brms)
library(cowplot)

# Other parameters
digits = 2
```

```{r get_path}
subjectName <- "newtest1/"

path <- paste0(subjectName, "S001/")
```

# Summary
Overall, I think the data of the participant is very encouraging. a) there was clear learning and b) the movement also seems to be varied and long enough. With the current set-up (84 trials) we get circa 14 minutes of movement from the task. 

# Trial results
```{r load_trial_results}
trialResults <- read.table(paste0(path, "trial_results.csv"), header = TRUE, sep = ",")
```

## Evidence for learning
### Learning curve
```{r learning_curves, fig.height = 3, fig.width = 8}
# Only consider data after the first block and convert into factor for visualisation
trialResults_afterBlock1                             <- trialResults[trialResults$block_num > 1, ]
trialResults_afterBlock1$Object                      <- trialResults_afterBlock1$objectName
trialResults_afterBlock1$timesObjectPresented_factor <- as.factor(trialResults_afterBlock1$timesObjectPresented)

distCurve <- ggplot(trialResults_afterBlock1, aes(x = timesObjectPresented_factor , y = euclideanDistance)) + 
  geom_point(aes(group = Object, colour = Object)) + 
  geom_line(aes(group = Object, colour = Object)) +
  labs(title = "Placement error", x = "Number of times the object was presented", y = "Euclidean distance in vm")

# Get legend and then remove from plot
plot_legend <- get_legend(distCurve)
distCurve <- distCurve + theme(legend.position = 'none')

timeCurve <- ggplot(trialResults_afterBlock1, aes(x = timesObjectPresented_factor, y = navTime)) + 
  geom_point(aes(group = Object, colour = Object)) + 
  geom_line(aes(group = Object, colour = Object)) +
  labs(title = "Navigation time", x = "Number of times the object was presented", y = "Time until response given in seconds")  + 
  theme(legend.position = 'none')

# Plot the two curves with the legend
plot_grid(distCurve, timeCurve, plot_legend, ncol = 3, rel_widths = c(3, 3, 1))
```

We clearly see that both the placement error (i.e. Euclidean distance) as well as the navigation time (i.e. time until the object location was retrieved) decrease over time. Block 1, where the object is immediately visible was excluded. 


### Compare Block 2 with Block 7
```{r block_comparison, fig.height = 3, fig.width = 8}
# Pick only Block 2 and 7
trialResults_block_comparison <- trialResults_afterBlock1[trialResults_afterBlock1$block_num == "2" | trialResults_afterBlock1$block_num == "7" , ]

# Convert block to factor
trialResults_block_comparison$block_factor <- as.factor(trialResults_block_comparison$block_num)

distBoxplot <- ggplot(trialResults_block_comparison, aes(x = block_factor, y = euclideanDistance, fill = block_factor)) + 
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(height = 0, width = 0.2) +
  theme(legend.position = 'none') +
  labs(title = "Placement error", x = "Block", y = "Euclidean distance in vm")

timeBoxplot <- ggplot(trialResults_block_comparison, aes(x = block_factor, y = navTime, fill = block_factor)) + 
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(height = 0, width = 0.2) +
  theme(legend.position = 'none') +
  labs(title = "Navigation time", x = "Block", y = "Time until response given in seconds") 

plot_grid(distBoxplot, timeBoxplot, ncol = 2)
```

We have the same pattern when we compare the performance of Block 2 with Block 7. 

### Modelling learning
Next, I thought a bit how we can model the data. Doeller et al. (2010) simply correlated 1/(mean of subject) with the grid score but I don't think that this is good solution.  

```{r fit_models, include = FALSE}
# Prepare data 
df  <- trialResults_afterBlock1

bm1_gamma    <- brm(euclideanDistance ~ timesObjectPresented, 
                    data = df, 
                    iter = 16000, 
                    warmup = 3000,
                    family = Gamma, 
                    seed = 12, 
                    save_pars = save_pars(all = TRUE))
bm1_gaussian <- brm(euclideanDistance ~ timesObjectPresented, 
                    data = df, 
                    iter = 16000, 
                    warmup = 3000,
                    family = gaussian, 
                    seed = 12, 
                    save_pars = save_pars(all = TRUE))

bf1 <- bayes_factor(bm1_gamma, bm1_gaussian)

# https://cran.r-project.org/web/packages/brms/vignettes/brms_monotonic.html
# https://rpubs.com/jwesner/gamma_glm
```

```{r conditional_effects1, include = FALSE}
# Get conditional effects
result1 <- plot(conditional_effects(bm1_gamma),points=T)$timesObjectPresented
result2 <- plot(conditional_effects(bm1_gaussian),points=T)$timesObjectPresented

# Add labels 
result1 <- result1 + labs(title = "Placement error with Gamma", x = "Number of times the object was presented", y = "Euclidean distance in vm")
result2 <- result2 + labs(title = "Placement error with Gaussian", x = "Number of times the object was presented", y = "Euclidean distance in vm")
```

```{r conditional_effects2, fig.height = 3, fig.width = 8}
# Plot both
plot_grid(result1, result2)
```

That is why I modeled the learning curve 1) with Gamma distribution and 2) with Gaussian distribution. While Bayes factor comparing both models favours the one with the Gaussian distribution I don't think it is the correct one a) because we know that theoretically it is wrong and b) because of the posterior checks:


#### Posterior predictive checks
```{r PPC, fig.height = 3, fig.width = 8}
ppc1 <- pp_check(bm1_gamma) + labs(title = 'Model using Gamma function')
ppc2 <- pp_check(bm1_gaussian) + labs(title = 'Model using Gaussian function')

plot_grid(ppc1, ppc2)
```

The comparison of the samples with the real data shows the Gamma distribution (see left) does a much better job accounting for the data. There more options to explore (e.g. modelling whether the predictor should be treated as continuous or monotone) as more data comes in.

In general, I think a good way to deal with this data would be a hierarchical model with subject as a random factor using a Gamma distribution. We can then use the slope and intercept to correlate them with the grid-coherence. 

```{r objectLocations}
true_objectLocations      <- ddply(trialResults_afterBlock1, c("objectName"), summarise, x = mean(target_x), z = mean(target_z))
```


# Position tracker
```{r get_position_tracker}
# Get all files
path2tracker <- paste0(path, "trackers/")
allTrackers  <- list.files(path2tracker)
n            <- length(allTrackers)

# First manually then loop
  temp       <- read.table(paste0(path2tracker, allTrackers[1]), header = TRUE, sep = ",") 
  temp$trial <- 1
  temp$time  <- temp$time - min(temp$time)
  data_FPC   <- temp

# Loop
for(i in 2:n){
  temp       <- read.table(paste0(path2tracker, allTrackers[i]), header = TRUE, sep = ",") 
  temp$trial <- i
  temp$time  <- temp$time - min(temp$time)
  
  # Bind
  data_FPC <- rbind(data_FPC, temp)
}
  
# Create proper boolean
data_FPC$moving <- ifelse(data_FPC$moving == "True", TRUE, FALSE)

# Get movement only 
data_FPC_movementOnly <- data_FPC[data_FPC$moving, ]
```

## Polar histogram of heading angle during movement
```{r polar_hist, fig.height = 6, fig.width = 6}
ggplot(data_FPC_movementOnly, aes(x = rot_y )) +
  geom_histogram(binwidth = 7.5, boundary = -7.5, colour = "black", size = .25) +
  scale_x_continuous(limits = c(0,360),
                     breaks = seq(0, 360, by = 60),
                     minor_breaks = seq(0, 360, by = 15))  +
  coord_polar() +
  labs(title = 'Polar histogram of player rotation during movement',
       y = 'Count',
       x = '')
```

Overall the heading angle looks fairly uniform but I didn't find any plots to compare our data with so it is heard to tell. 

## Show position on 2D plane per time
```{r path_data, fig.height = 6, fig.width = 6}
circleFun <- function(center = c(0,0),diameter = 1, npoints = 100){
    r = diameter / 2
    tt <- seq(0,2*pi,length.out = npoints)
    xx <- center[1] + r * cos(tt)
    yy <- center[2] + r * sin(tt)
    return(data.frame(x = xx, y = yy))
}

dat <- circleFun(c(0,-0), 180 ,npoints = 100)


ggplot(data_FPC, aes(x = pos_x , y = pos_z)) + 
  geom_path(alpha = 0.5, mapping = aes(group = trial), size = 1) + 
  coord_cartesian(ylim = c(-90, 90), xlim = c(-90, 90)) +
  geom_path(data = dat, mapping = aes(x = x, y = y)) +
  geom_point(data = trialResults_afterBlock1, mapping = aes(x = end_x, y = end_z, colour = objectName), size = 4, alpha = 0.5) +
  geom_point(data = true_objectLocations, mapping = aes(x = x, y = z, colour = objectName), size = 6) +
  theme(legend.position = 'none') +
  labs(x = "Position on x-axis", y = "Position on z-axis")
``` 

The figure above illustrates the individual trajectories, true object locations (larger non-transparent colour points) and the retrieved locations (smaller transparent colour points).


## How long did the participant spend moving? 
```{r movementPerTrial}
# Calculate movement per trial
movementPerTrial <- ddply(data_FPC, c('trial'), summarise, start = min(time), end = max(time), moving = sum(moving), total = length(time))
movementPerTrial$movementTime  <- (movementPerTrial$end/movementPerTrial$total)*movementPerTrial$moving
```

```{r movementPerTrial_hist}
ggplot(movementPerTrial, aes(x = movementTime)) + geom_histogram() + labs(title = "How many seconds did the participant move per trial?",
                                                                          x = "Movement time in seconds",
                                                                          y = "Count")
```

An important question for the fMRI-version of the experiment is how long per trial did the participant spend on moving forward, which was on average `r round(mean(movementPerTrial$movementTime), 2)` seconds movement per trial.

# Other information
```{r load_log_entries}
# The last column needs to be removed. 
log_entries <- read.table("alex/S001/session_info/log.csv", header = TRUE, sep = ",")
```

```{r cue_period}
# Load library stringr
library("stringr")
 
# Filter data with str_detect for strings
# containing "Cue"
cue_only <- log_entries[str_detect(log_entries$message, "Cue"), ]

# Calculate how long the cue periods are
cue_only$trial <- rep(1:(nrow(cue_only)/2), each = 2)
cue_periods    <- ddply(cue_only, c('trial'), summarise, cue_period = timestamp[2] - timestamp[1])

# Add to data.frame
questions <- data.frame(Description = "How long were the estimated cue periods?", 
                        Value = round(mean(cue_periods$cue_period), digits = digits),
                        Measure = "seconds")
```


```{r delay_period}
# Filter data with str_detect for strings
# containing "delay"
delay_only <- log_entries[str_detect(log_entries$message, "delay"), ]

# Calculate how long the cue periods are
delay_only$trial <- rep(1:(nrow(delay_only)/2), each = 2)
delay_periods    <- ddply(delay_only, c('trial'), summarise, delay_period = timestamp[2] - timestamp[1])

# Add to data.frame
questions <- rbind(questions,
                   data.frame(Description = "How long were the estimated delay periods?", 
                              Value = round(mean(delay_periods$delay_period), digits = digits),
                              Measure = "seconds"))
```

```{r trial_lengths}
# Get average trial length for block > 1
trial_length <- ddply(trialResults_afterBlock1, c('trial_num'), summarise, duration = end_time  - start_time)

# Add to data.frame
questions <- rbind(questions,
                   data.frame(Description = "How long was the average trial?", 
                              Value = round(mean(trial_length$duration), digits = digits),
                              Measure = "seconds"))
```

```{r total_movement}
# Add to data.frame
questions <- rbind(questions,
                   data.frame(Description = "How long was the total time spend moving?", 
                              Value = round(sum(movementPerTrial$movementTime)/60, digits = digits),
                              Measure = "minutes"))
```

```{r fps}
df_Hz <- ddply(data_FPC, c('trial'), summarise, 
               trial_duration = max(time) - min(time),
               samples = length(time))

# Calculate frequency
df_Hz$Hz <- df_Hz$samples/df_Hz$trial_duration

# Add to data.frame
questions <- rbind(questions,
                   data.frame(Description = "What was the average Frames Per Second?", 
                              Value = round(mean(df_Hz$Hz)),
                              Measure = "fps"))
```

```{r speed}
# Extract values
x     <- data_FPC_movementOnly$pos_x
y     <- 2
z     <- data_FPC_movementOnly$pos_z
times <- data_FPC_movementOnly$time
n     <- length(times)

# Calculate
distance       <- euclideanDistance3D(x[1:(n - 1)], y, z[1:(n - 1)], x[2:n], y, z[2:n])
timeDifference <- times[2:n] - times[1:(n - 1)]
speed          <- c(NA, distance/timeDifference)

# Replace with NA when new trial with run length encoding
trial_rle     <- rle(data_FPC_movementOnly$trial)
trial_changes <- cumsum(trial_rle$lengths)[1:(length(trial_rle$lengths) - 1)]  + 1 # because it need to be shifted by 1 to get the first data point of the trial
speed[trial_changes] <- NA
data_FPC_movementOnly$speed <-  speed # Add NA for first value

# Add to data.frame
questions <- rbind(questions,
                   data.frame(Description = "What was the average speed?", 
                              Value = round(mean(data_FPC_movementOnly$speed, na.rm = TRUE), digits = digits),
                              Measure = "vm/s"))
```

```{r table}
kable(questions)
```
